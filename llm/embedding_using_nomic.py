import re
from langchain_core.documents import Document
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma

# --- Configuration ---
# This is the single, combined text file generated by your other script.
SOURCE_TEXT_FILE = "ALL14TEXT.txt" 
# The directory where the new, combined vector database will be stored.
PERSIST_DIRECTORY = "./chroma_all_db"
# The embedding model to use.
EMBEDDING_MODEL = "nomic-embed-text"

def create_vector_db():
    """
    Loads the combined text file, splits it into documents with proper metadata,
    and stores them in a Chroma vector database.
    """
    # --- Step 1: Load the entire file content ---
    try:
        with open(SOURCE_TEXT_FILE, "r", encoding="utf-8") as f:
            full_content = f.read()
    except FileNotFoundError:
        print(f"‚ùå Error: The file '{SOURCE_TEXT_FILE}' was not found.")
        print("Please run the 'process_all_pptx.py' script first to generate it.")
        return

    # --- Step 2: Split the content by the custom slide separator ---
    # The regex now looks for the new, more descriptive separator.
    # Using a capturing group `()` in re.split keeps the separators in the list.
    separator_pattern = r'(--- Source: .*?\.pptx, Slide: \d+ ---)'
    parts = re.split(separator_pattern, full_content)

    documents = []
    # The list 'parts' will be structured as: ['', 'separator1', 'content1', 'separator2', 'content2', ...]
    # We iterate through it, taking a separator and its corresponding content in pairs.
    for i in range(1, len(parts), 2):
        separator = parts[i]
        content = parts[i+1]

        # --- Step 3: Parse the separator to extract metadata ---
        # This regex extracts the filename and slide number from the separator string.
        meta_match = re.search(r'Source: (.*?\.pptx), Slide: (\d+)', separator)
        
        if meta_match:
            source_filename = meta_match.group(1)
            slide_number = int(meta_match.group(2))
            
            # Create a LangChain Document with the extracted content and metadata
            doc = Document(
                page_content=content.strip(),
                metadata={
                    "source": source_filename,
                    "slide": slide_number
                }
            )
            documents.append(doc)

    if not documents:
        print("Could not find any documents to process. Check the format of your text file.")
        return
        
    print(f"‚úÖ Created {len(documents)} logical documents from '{SOURCE_TEXT_FILE}'.")

    # --- Step 4: Embed and Store in Chroma DB ---
    print(f"Embedding documents using '{EMBEDDING_MODEL}'...")
    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)

    # Create and persist the vector store from the documents
    vectorstore = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        persist_directory=PERSIST_DIRECTORY
    )
    vectorstore.persist()

    print(f"üéâ Vector database created and stored in '{PERSIST_DIRECTORY}'.")

# --- Main Execution ---
if __name__ == "__main__":
    create_vector_db()
